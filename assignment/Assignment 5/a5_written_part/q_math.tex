\titledquestion{Attention exploration}[22]
\label{sec:analysis}

Multi-headed self-attention is the core modeling component of Transformers.
In this question, we'll get some practice working with the self-attention equations, and motivate why multi-headed self-attention can be preferable to single-headed self-attention.

Recall that attention can be viewed as an operation on a \textit{query} $q\in\mathbb{R}^d$, a set of \textit{value} vectors $\{v_1,\dots,v_n\}, v_i\in\mathbb{R}^d$, and a set of \textit{key} vectors $\{k_1,\dots,k_n\}, k_i \in \mathbb{R}^d$, specified as follows:
\begin{align}
    &c = \sum_{i=1}^{n} v_i \alpha_i \\
    &\alpha_i = \frac{\exp(k_i^\top q)}{\sum_{j=1}^{n} \exp(k_j^\top q)}
\end{align}
with $\alpha_i$ termed the ``attention weights''.
Observe that the output $c\in\mathbb{R}^d$ is an average over the value vectors weighted with respect to $\alpha_i$.

\begin{parts}

    \part[4] \textbf{Copying in attention.} One advantage of attention is that it's particularly easy to ``copy'' a value vector to the output $c$. In this problem, we'll motivate why this is the case.

    \begin{subparts}
        \subpart[1]
        \textbf{Explain} why $\alpha$ can be interpreted as a categorical probability distribution.
        \begin{answer}
            The \textit{key} vectors correspond to $n$ possible \textbf{discrete} categories.\\
            Since $\alpha_i = \frac{\exp(k_i^\top q)}{\sum_{j=1}^{n} \exp(k_j^\top q)}$, $0 \leq \alpha_i \leq 1$ must hold.
            In addition, $\sum_{i=1}^{n} \alpha_i = 1$.\\
            Therefore, $\alpha$ can be interpreted as a categorical probability distribution.
        \end{answer}
        \subpart[2] The distribution $\alpha$ is typically relatively ``diffuse''; the probability mass is spread out between many different $\alpha_i$. However, this is not always the case. \textbf{Describe} (in one sentence) under what conditions the categorical distribution $\alpha$ puts almost all of its weight on some $\alpha_j$, where $j \in \{1, \ldots, n\}$ (i.e. $\alpha_j \gg \sum_{i \neq j} \alpha_i$). What must be true about the query $q$ and/or the keys $\{k_1,\dots,k_n\}$?
        \begin{answer}
            $k_j^\top q \gg k_i^\top q $ must hold for any $i$, where $i \in \{1,\ldots,n\} \land i \neq j$.
        \end{answer}
        \subpart[1] Under the conditions you gave in (ii),  \textbf{describe} what properties the output $c$ might have.
        \begin{answer}
            $c \approx v_j$, where $\alpha_j \gg \sum_{i \neq j} \alpha_i$.
        \end{answer}
        \subpart[1] \textbf{Explain} (in two sentences or fewer) what your answer to (ii) and (iii) means intuitively. \\
        \begin{answer}
            If the \textit{query} $q$ is very similar to a single \textit{key} vector and almost orthogonal to the rest of them,
            the attention output $c$ could be almost identical to the corresponding \textit{value} vector of this \textit{key} vector,
            as if ``copied" directly.
        \end{answer}

    \end{subparts}


    \part[7]\textbf{An average of two.}
    \label{q_avg_of_two}
    Instead of focusing on just one vector $v_j$, a Transformer model might want to incorporate information from \textit{multiple} source vectors.
    Consider the case where we instead want to incorporate information from \textbf{two} vectors $v_a$ and $v_b$, with corresponding key vectors $k_a$ and $k_b$.
    \begin{subparts}
        \subpart[3] How should we combine two $d$-dimensional vectors $v_a, v_b$ into one output vector $c$ in a way that preserves information from both vectors?
        In machine learning, one common way to do so is to take the average: $c = \frac{1}{2} (v_a + v_b)$.
        It might seem hard to extract information about the original vectors $v_a$ and $v_b$ from the resulting $c$, but under certain conditions one can do so. In this problem, we'll see why this is the case.
        \\ \\
        Suppose that although we don't know $v_a$ or $v_b$, we do know that $v_a$ lies in a subspace $A$ formed by the $m$ basis vectors $\{a_1, a_2, \ldots, a_m\}$, while $v_b$ lies in a subspace $B$ formed by the $p$ basis vectors $\{b_1, b_2, \ldots, b_p\}.$ (This means that any $v_a$ can be expressed as a linear combination of its basis vectors, as can $v_b$. All basis vectors have norm 1 and orthogonal to each other.)
        Additionally, suppose that the two subspaces are orthogonal; i.e. $a_j^\top b_k = 0$ for all $j, k$.

        Using the basis vectors $\{a_1, a_2, \ldots, a_m\}$, construct a matrix $M$ such that for arbitrary vectors $v_a \in A$ and $v_b \in B$, we can use $M$ to extract $v_a$ from the sum vector $s = v_a + v_b$. In other words, we want to construct $M$ such that for any $v_a, v_b$,  $Ms = v_a$).

        \textbf{Note:} both $M$ and $v_a, v_b$ should be expressed as a vector in $\mathbb{R}^d$, not in terms of vectors from $A$ and $B$. \\

        \textbf{Hint:} Given that the vectors $\{a_1, a_2, \ldots, a_m\}$ are both \textit{orthogonal} and \textit{form a basis} for $v_a$, we know that there exist some $c_1, c_2, \ldots, c_m$ such that $v_a = c_1 a_1 + c_2 a_2 + \cdots + c_m a_m$. Can you create a vector of these weights $c$?

        \begin{answer}
            \newline
            We have $a_j^\top v_a = c_1 a_j^\top a_1 + \cdots + c_j a_j^\top a_j + \cdots + c_m a_j^\top a_m = c_j$ and
            $a_j^\top v_b = d_1 a_j^\top b_1 + \cdots + d_m a_j^\top b_p = 0$, where $1 \leq j \leq m$. Let $M =
            \begin{bmatrix}
                a_1^\top \\
                \vdots   \\
                a_m^\top
            \end{bmatrix}
            $, we have $M s = M (v_a + v_b) = v_a$.
        \end{answer}

        \subpart[4] As before, let $v_a$ and $v_b$ be two value vectors corresponding to key vectors $k_a$ and $k_b$, respectively.
        Assume that (1) all key vectors are orthogonal, so $k_i^\top k_j = 0$ for all $i \neq j$; and (2) all key vectors have norm $1$.\footnote{Recall that a vector $x$ has norm 1 iff $x^\top x = 1$.}
        \textbf{Find an expression} for a query vector $q$ such that $c \approx \frac{1}{2}(v_a + v_b)$. \footnote{Hint: while the softmax function will never \textit{exactly} average the two vectors, you can get close by using a large scalar multiple in the expression.}
        \begin{answer}
            \newline
            We want $\alpha_1 \approx \alpha_2 \approx \frac{1}{2}$, so the attention scores $k_a^\top q$ and $k_b^\top q$
            must be close and their sum must be dominating.
            So $q = \beta (k_a + k_b)$, where $\beta \gg 0$ (large scalar multiple).
        \end{answer}
    \end{subparts}

    \part[5]\textbf{Drawbacks of single-headed attention:} \label{q_problem_with_single_head}
    In the previous part, we saw how it was \textit{possible} for a single-headed attention to focus equally on two values.
    The same concept could easily be extended to any subset of values.
    In this question we'll see why it's not a \textit{practical} solution.
    Consider a set of key vectors $\{k_1,\dots,k_n\}$ that are now randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i \in \mathbb{R}^d$ are known to you, but the covariances $\Sigma_i$ are unknown.
    Further, assume that the means $\mu_i$ are all perpendicular; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.

    \begin{subparts}
        \subpart[2] Assume that the covariance matrices are $\Sigma_i = \alpha I \forall i \in \{1, 2, \ldots, n\}$, for vanishingly small $\alpha$.
        Design a query $q$ in terms of the $\mu_i$ such that as before, $c\approx \frac{1}{2}(v_a + v_b)$, and provide a brief argument as to why it works.
        \begin{answer}
            It is obvious that $k_i \approx \mu_i$ here since $\alpha$ is \textit{vanishingly small}.
            So $q = \beta (k_a + k_b)$, where $\beta \gg 0$.
        \end{answer}

        \subpart[3] Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue. Specifically, in some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\mu_a$. As an example, let us consider a covariance for item $a$ as $\Sigma_a = \alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$ (as shown in figure \ref{ka_plausible}). This causes $k_a$ to point in roughly the same direction as $\mu_a$, but with large variances in magnitude. Further, let $\Sigma_i = \alpha I$ for all $i \neq a$.
        \begin{figure}[h]
            \centering
            \captionsetup{justification=centering,margin=2cm}
            \includegraphics[width=0.35\linewidth]{images/ka_plausible.png}
            \caption{The vector $\mu_a$ (shown here in 2D as an example), with the range of possible values of $k_a$ shown in red. As mentioned previously, $k_a$ points in roughly the same direction as $\mu_a$, but may have larger or smaller magnitude.}
            \label{ka_plausible}
        \end{figure}

        When you sample $\{k_1,\dots,k_n\}$ multiple times, and use the $q$ vector that you defined in part i., what qualitatively do you expect the vector $c$ will look like for different samples?

        \begin{answer}
            \\
            Since $\alpha$ is vanishingly small, we may \textbf{assume} $k_a \approx \gamma \mu_a$, where $\gamma \sim \mathcal{N}(1, \frac{1}{2})$.
            Also, $k_i^\top q \approx 0$ still holds for $i \notin \{a, b\}$, and $k_b^\top q = \beta \gg 0$.
            Therefore, \\
            \[
                \begin{aligned}
                    c &\approx \frac{\exp (\gamma \beta)}{\exp(\gamma \beta) + \exp(\beta)} v_a + \frac{\exp (\beta)}{\exp(\gamma \beta) + \exp(\beta)} v_b \\
                    &= \frac{1}{\exp((1-\gamma) \beta) + 1} v_a + \frac{1}{\exp((\gamma - 1) \beta) + 1} v_b \\
                \end{aligned}
            \]
            So $c$ will oscillates between $v_a$ and $v_b$ as we sample $k_i$ multiple times.
        \end{answer}
    \end{subparts}

    \part[3] \textbf{Benefits of multi-headed attention:}
    Now we'll see some of the power of multi-headed attention.
    We'll consider a simple version of multi-headed attention which is identical to single-headed self-attention as we've presented it in this homework, except two query vectors ($q_1$ and $q_2$) are defined, which leads to a pair of vectors ($c_1$ and $c_2$), each the output of single-headed attention given its respective query vector.
    The final output of the multi-headed attention is their average, $\frac{1}{2}(c_1+c_2)$.
    As in question 1(\ref{q_problem_with_single_head}), consider a set of key vectors $\{k_1,\dots,k_n\}$ that are randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i$ are known to you, but the covariances $\Sigma_i$ are unknown.
    Also as before, assume that the means $\mu_i$ are mutually orthogonal; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.
    \begin{subparts}
        \subpart[1]
        Assume that the covariance matrices are $\Sigma_i=\alpha I$, for vanishingly small $\alpha$.
        Design $q_1$ and $q_2$ such that $c$ is approximately equal to $\frac{1}{2}(v_a+v_b)$.

        \begin{answer}
            Since $\alpha$ is vanishingly small, $k_i \approx \mu_i$.
            So $q_1 = q_2 = \beta (k_a + k_b)$ , where $\beta \gg 0$.
        \end{answer}

        \subpart[2]
        Assume that the covariance matrices are $\Sigma_a=\alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$, and $\Sigma_i=\alpha I$  for all $i \neq a$.
        Take the query vectors $q_1$ and $q_2$ that you designed in part i.
        What, qualitatively, do you expect the output $c$ to look like across different samples of the key vectors? Please briefly explain why. You can ignore cases in which $k_a^\top q_i < 0$.

        \begin{answer}
            Since $\alpha$ is vanishingly small, we may \textbf{assume} $k_a \approx \gamma \mu_a$, where $\gamma \sim \mathcal{N}(1, \frac{1}{2})$.
            Also, $k_j^\top q_i \approx 0$ still holds for $i \in \{1, 2\}$ and $j \notin \{a, b\}$, and $k_b^\top q_i = \beta \gg 0$.
            Therefore, \\
            \[
                \begin{aligned}
                    c &\approx \frac{\exp (\gamma \beta)}{\exp(\gamma \beta) + \exp(\beta)} v_a + \frac{\exp (\beta)}{\exp(\gamma \beta) + \exp(\beta)} v_b \\
                    &= \frac{1}{\exp((1-\gamma) \beta) + 1} v_a + \frac{1}{\exp((\gamma - 1) \beta) + 1} v_b \\
                \end{aligned}
            \]
            As we now have a $c$ for each attention head, it is equivalent to move $\gamma$ close to its expectation, which is $1$.
            So $c$ will approach $\frac{1}{2}(v_a + v_b)$ as we increase the number of attention heads.
        \end{answer}


    \end{subparts}
\end{parts}